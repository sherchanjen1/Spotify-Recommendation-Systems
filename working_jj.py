# -*- coding: utf-8 -*-
"""working jj

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13-lQq7z9WlDWt4S2PVhwT_DJd-FnvrWP
"""

# Basic Data Processing and Analysis
import pandas as pd
import numpy as np
import re

# Visualization Libraries
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from collections import Counter

# For interactive visualizations
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Machine Learning Libraries
from sklearn import __version__ as sklearn_version
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
from sklearn.model_selection import train_test_split


# Advanced ML Models
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.cluster import KMeans

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Natural Language Processing
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
!pip install unidecode
import unidecode

# Network Analysis
import networkx as nx

# Time Series Analysis
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.seasonal import seasonal_decompose

# System and Settings
import warnings
warnings.filterwarnings('ignore')

# Display Settings
pd.set_option('display.max_columns', None)

# Set basic plotting styles
sns.set_theme()
plt.style.use('default')

# For Plotly in Colab
from plotly.offline import iplot, init_notebook_mode
init_notebook_mode(connected=True)

# Print versions for reference
import sys
print(f"Python version: {sys.version.split()[0]}")
print(f"Pandas version: {pd.__version__}")
print(f"Numpy version: {np.__version__}")
print(f"Tensorflow version: {tf.__version__}")
print(f"Scikit-learn version: {sklearn_version}")


# For NLP problem statement
import nltk
nltk.download('punkt_tab')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# Importing dataset: a button named "choose files" will appear.  click on the button and choose our dataset called "dataset.csv"
from google.colab import files
uploaded = files.upload()

# After uploading, read the file
df_original = pd.read_csv(next(iter(uploaded)), encoding='utf-8')

df_v1 = df_original.copy()
df_v1.head(10)

df_v1.describe()

#Get a list of all variables, number of counts on each variable and datatype
df_v1.info()

#Data Cleaning: removed track_id, instrumentalness, and Unnamed: 0 since it does not contain useful data and not necessary for analytical purposes.
#with these removed we can find more duplicates.
df_v2_remove_vars = df_v1.copy()
df_v2_remove_vars = df_v2_remove_vars.drop(columns=['track_id', 'Unnamed: 0', 'album_name', 'key', 'mode', 'time_signature', 'liveness'])
df_v2_remove_vars.info()

#Data Cleaning: count the number of rows with zero popularity
df_v2_remove_vars[df_v2_remove_vars['popularity'] == 0].shape[0]

#Data Cleaning:
df_v2_rem_pop_0 = df_v2_remove_vars.copy()
df_v2_rem_pop_0 =df_v2_rem_pop_0[df_v2_rem_pop_0['popularity'] != 0]
df_v2_rem_pop_0

df_v2_rem_pop_0.info()

df_v2_rem_pop_0.describe()

#Data Cleaning:  For consisteny for all text has been
#standardized by changing al text to lowercase as certain ML are case
#case sensitive and helps reduce noise and improve accuracy
#on text models
df_v3_lowercase = df_v2_rem_pop_0.copy()
df_v3_lowercase = df_v3_lowercase.applymap(lambda x: x.lower() if isinstance(x, str) else x)
df_v3_lowercase

#Data Cleaning: Check for any duplicates.  True being duplicates
df_v4_row_duplicate = df_v3_lowercase.copy()
df_v4_row_duplicate = df_v4_row_duplicate.duplicated()
df_v4_row_duplicate.value_counts()

#Data Cleaning: Check for any duplicates.  True being duplicates
df_v4_rem_duplicate = df_v3_lowercase.copy()

df_v4_rem_duplicate = df_v4_rem_duplicate.drop_duplicates()
df_v4_rem_duplicate.describe()

#Data Cleaning: Check for any NaN and missing values.
df_v4_ck_nan = df_v4_rem_duplicate.copy()
df_v4_ck_nan.isnull().sum()

#Data Cleaning: Checking if there are any ASCII text as these skew any NLP.

def find_non_ascii_except_extended_latin_and_text(text):
    # Remove allowed special characters from the text
    text = re.sub(r"[()\[\]{}<>_\-]", "", str(text))
    # Check for characters outside the extended Latin range
    for char in text:
        if ord(char) > 255:  # Check for non-ASCII characters
            return True  # Found a non-allowed character
    return False  # All characters are within the allowed range


# from the dataframe get columns
columns_to_check = ['track_name', 'artists']

# Apply the function to the specified columns
for column in columns_to_check:
    df_v4_rem_duplicate[f'{column}_non_ascii_check'] = df_v4_rem_duplicate[column].apply(find_non_ascii_except_extended_latin_and_text)

# Filter rows based on the checks in both columns
filtered_rows = df_v4_rem_duplicate[
    (df_v4_rem_duplicate['track_name_non_ascii_check']) | (df_v4_rem_duplicate['artists_non_ascii_check'])
]

# Print or further process the filtered rows
print(filtered_rows)

#Data Cleaning: Removing non ascii except extended latin and text
df_v5_rem_duplicate = df_v4_rem_duplicate.drop(filtered_rows.index)
df_v5_rem_duplicate

# Remove the temporary check columns HERE
df_v5_rem_duplicate = df_v5_rem_duplicate.drop(columns=['track_name_non_ascii_check', 'artists_non_ascii_check'])
df_v5_rem_duplicate

#Check each row to see if there are any ASCII.
def count_non_ascii(text):
    # Remove allowed special characters from the text
    text = re.sub(r"[()\[\]{}<>_\-]", "", str(text))
    # Check for characters outside the extended Latin range
    for char in text:
        if ord(char) > 255:  # Check for non-ASCII characters
            return True  # Found a non-allowed character
    return False  # All characters are within the allowed range

# Apply the function to count non-ASCII characters in each cell
non_ascii_counts_by_row = df_v5_rem_duplicate.applymap(lambda x: count_non_ascii(x) if isinstance(x, str) else 0)

# Calculate the total number of non-ASCII characters per row
non_ascii_counts_by_row['total_non_ascii'] = non_ascii_counts_by_row.sum(axis=1)

# Sort the DataFrame by the 'total_non_ascii' column in ascending or descending order
# To sort in ascending order, use ascending=True (default)
# To sort in descending order, use ascending=False
sorted_non_ascii_counts = non_ascii_counts_by_row.sort_values(by='total_non_ascii', ascending=True)

# Output the sorted result
print("Non-ASCII counts by row (sorted):")
print(sorted_non_ascii_counts[['total_non_ascii']])

# filtered_df = df[df['track_name'].str.contains("love", case=False, na=False)]

# Define the special characters to search for NLP
special_chars = ["@", "$", "&", "w."]

# Assuming your DataFrame is named 'df'
columns_to_check = ['track_name', 'artists']

# Create a boolean mask to identify rows with special characters, using the correct index
mask = pd.Series(False, index=df_v5_rem_duplicate.index)  # Initialize with False and the correct index

for column in columns_to_check:
    for char in special_chars:
        mask |= df_v5_rem_duplicate[column].str.contains(char, regex=False)  # Update mask if any special character is found

# Filter the DataFrame using the mask
df_v6_check_spcl_char = df_v5_rem_duplicate[mask]

# Print or further process the filtered rows
print(df_v6_check_spcl_char)

df_v6_replace_spcl_char = df_v5_rem_duplicate.copy()
special_chars = {
    "@": "a",
    "$": "s",
    "&": "and",
    "w.": "with",
    "?": "",
    '"': "",
    '#': "",
    '%': "",
    "!": ""
}
def replace_characters(input_string, special_chars):
  for orig_char, repl_char in special_chars.items():
    input_string = input_string.replace(orig_char, repl_char)
  return input_string

df_v6_replace_spcl_char['track_name'] = df_v6_replace_spcl_char['track_name'].apply(lambda x: replace_characters(x, special_chars))
df_v6_replace_spcl_char['artists'] = df_v6_replace_spcl_char['artists'].apply(lambda x: replace_characters(x, special_chars))
df_v6_replace_spcl_char

filtered_df2 = df_v6_replace_spcl_char[df_v6_replace_spcl_char['track_name'].str.contains('#', case=False, na=False)]
filtered_df2

#Check version 5 summary description
#The data count has decreaAsed down to 105,788
df_v6_replace_spcl_char.describe()

df_v6_replace_spcl_char.info()

df_v7_replace_duration_to_min = df_v6_replace_spcl_char
# Replace 'duration_ms' with its equivalent in minutes
df_v7_replace_duration_to_min['duration_ms'] = (df_v7_replace_duration_to_min['duration_ms'] / 60000)

# Rename the column to reflect the new unit
df_v7_replace_duration_to_min.rename(columns={'duration_ms': 'duration_mins'}, inplace=True)
df_v7_replace_duration_to_min.head(10)

df_v8_tempo_label = df_v7_replace_duration_to_min.copy()
# Define bins for categorizing tempo
bins = [df_v8_tempo_label['tempo'].min() - 1, 60, 120, df_v7_replace_duration_to_min['tempo'].max() + 1]
labels = ['low', 'medium', 'high']

# Create the tempo_category column
df_v8_tempo_label['tempo_category'] = pd.cut(df_v8_tempo_label['tempo'], bins=bins, labels=labels)

#Display the updated DataFrame
columns_to_check = ['tempo_category', 'tempo']
for column in columns_to_check:
    unique_values = df_v8_tempo_label[column].unique()
    print(f"Unique values in '{column}': {unique_values}")
#dfv5['tempo_category'].unique()

# Use this df if you want to work on the full genre.   be careful as these may have duplicates.
df_v9_clean_df = df_v8_tempo_label.copy()
df_v9_clean_df

#Data Cleaning find out the number of different unique values for
#variables with datatype that are objects and categories.
df_v9_clean_df.describe(include=['object', 'category'])

#AGGREGATING:  HAVE GENRE INTO LISTS/grouped

df_v11_genre_list = df_v9_clean_df.groupby(['track_name',	'artists',	'popularity',	'duration_mins',
                            'danceability',	'explicit',	'energy',	'loudness',
                            'speechiness',	'acousticness','valence',	'tempo'])['track_genre'].agg(list).reset_index()
df_v11_genre_list

# Create a copy of the original dataframe
df_v10_clean_standard = df_v9_clean_df.copy()

# Initialize the scaler
scaler = StandardScaler()

# List of features to standardize
features_to_standardize = [
    'popularity',
    'danceability',
    'energy',
    'loudness',
    'speechiness',
    'acousticness',
    'valence',
    'tempo'
]

# Create new standardized columns with '_std' suffix
for feature in features_to_standardize:
    # Reshape data for StandardScaler
    feature_reshaped = df_v10_clean_standard[feature].values.reshape(-1, 1)
    # Fit and transform the data
    feature_standardized = scaler.fit_transform(feature_reshaped)
    # Add new standardized column
    df_v10_clean_standard[f'{feature}_std'] = feature_standardized

# Verify the standardization
print("Verification of standardized features:")
print("\nMean should be close to 0 and std should be close to 1:")
for feature in features_to_standardize:
    print(f"\n{feature}_std:")
    print(f"Mean: {df_v10_clean_standard[f'{feature}_std'].mean():.6f}")
    print(f"Std: {df_v10_clean_standard[f'{feature}_std'].std():.6f}")

# Create a copy of the original dataframe
df_v12_genre_list_standard = df_v11_genre_list.copy()

# Initialize the scaler
scaler = StandardScaler()

# List of features to standardize
features_to_standardize = [
    'popularity',
    'danceability',
    'energy',
    'loudness',
    'speechiness',
    'acousticness',
    'valence',
    'tempo'
]

# Create new standardized columns with '_std' suffix
for feature in features_to_standardize:
    # Reshape data for StandardScaler
    feature_reshaped = df_v12_genre_list_standard[feature].values.reshape(-1, 1)
    # Fit and transform the data
    feature_standardized = scaler.fit_transform(feature_reshaped)
    # Add new standardized column
    df_v12_genre_list_standard[f'{feature}_std'] = feature_standardized

# Verify the standardization
print("Verification of standardized features:")
print("\nMean should be close to 0 and std should be close to 1:")
for feature in features_to_standardize:
    print(f"\n{feature}_std:")
    print(f"Mean: {df_v12_genre_list_standard[f'{feature}_std'].mean():.6f}")
    print(f"Std: {df_v12_genre_list_standard[f'{feature}_std'].std():.6f}")

df_v10_clean_standard = df_v10_clean_standard.round(2)
df_v10_clean_standard

df_v12_genre_list_standard = df_v12_genre_list_standard.round(2)
df_v12_genre_list_standard

#Dataframes to use:

df_v9_clean_df # df prior to standardized focuses on genre which will have duplicates
df_v10_clean_standard # df that is standardized focuses on genre which will have duplicates
df_v11_genre_list # df prior to standardized focuses grouping genre and fixes duplicates
df_v12_genre_list_standard #df that is standardized focuses grouping genre and fixes duplicates

# @title Final
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Initialize WCSS list
wcss = []

# Define the features you want to use for clustering
features = df_v10_clean_standard[['popularity_std', 'danceability_std', 'energy_std', 'loudness_std', 'valence_std', 'tempo_std']] # Example features

# Iterate over a range of cluster numbers (e.g., 1 to 10)
for k in range(1, 11):  # Experiment with a range
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(features)  # 'features' is your dataset for clustering
    wcss.append(kmeans.inertia_)  # Inertia is the WCSS

print(wcss)  # Debugging: See WCSS for each k

# Plot the Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.xticks(range(1, 11))
plt.show()

from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

features = df_v10_clean_standard[['popularity_std', 'danceability_std', 'energy_std', 'loudness_std', 'valence_std', 'tempo_std']]

silhouette_scores = []

# Iterate over a range of cluster numbers (e.g., 2 to 10)
for k in range(2, 11):  # Silhouette Score requires at least 2 clusters
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(features)  # Cluster labels
    score = silhouette_score(features, labels)  # Compute Silhouette Score
    silhouette_scores.append(score)

print(silhouette_scores)  # Debugging: See scores for each k

plt.figure(figsize=(8, 5))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--', color='b')
plt.title('Silhouette Scores for Different k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Select the musical features for correlation analysis
musical_features = ['popularity_std', 'danceability_std', 'energy_std',
                   'loudness_std', 'speechiness_std', 'acousticness_std',
                   'valence_std', 'tempo_std']

# Calculate the correlation matrix
correlation_matrix = df_v10_clean_standard[musical_features].corr()

# Create a heatmap to visualize the correlations
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of Musical Features')
plt.show()

# Group by genre and calculate sum of popularity
genre_popularity = df_v10_clean_standard.groupby('track_genre')['popularity'].sum().reset_index()
genre_popularity.columns = ['track_genre', 'total_popularity']

# Sort by total popularity in descending order
genre_popularity = genre_popularity.sort_values(by='total_popularity', ascending=False)

# Get the top N genres (you can adjust N as needed)
top_n = 20  # Example: Get the top 20 genres
top_genres = genre_popularity.head(top_n)['track_genre'].tolist()

# Create a new DataFrame with only the top genres
df_top_genres_old = df_v10_clean_standard[df_v10_clean_standard['track_genre'].isin(top_genres)]

# Display the new DataFrame
df_top_genres_old

# Apply K-means clustering
k = 4  # Set the number of clusters (you can experiment with this value)
kmeans = KMeans(n_clusters=k, random_state=42)

# Correctly call the fit_predict method with your data
df_top_genres_old['cluster'] = kmeans.fit_predict(df_top_genres_old[['tempo_std', 'danceability_std']])

##K-means Clustering of Tracks by Tempo and Danceability

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Assuming df_top_genres_old contains the necessary features and clusters
k = 3  # Number of clusters
kmeans = KMeans(n_clusters=k, random_state=42)

# Fit the K-means model
df_top_genres_old['cluster'] = kmeans.fit_predict(df_top_genres_old[['tempo_std', 'danceability_std']])

# Function to get recommendations based on track index, genre, and cluster
def recommend_tracks_by_genre_and_cluster(track_index, track_genre, df, num_recommendations=5):
    """
    Recommends tracks based on the genre and the cluster of a given track index.

    Args:
        track_index: The index of the track in the DataFrame.
        genre: The genre to filter the tracks.
        df: The DataFrame containing track data and clusters.
        num_recommendations: The number of recommendations to generate.

    Returns:
        A DataFrame containing the recommended tracks.
    """
    # Filter the DataFrame by the specified genre
    genre_filtered_df = df[df['track_genre'].str.lower() == track_genre.lower()]

    if genre_filtered_df.empty:
        return f"No tracks found for genre: {track_genre}"

    # Get the cluster of the input track using the index
    cluster = df.loc[df.index[track_index], 'cluster']

    # Filter tracks in the same cluster within the selected genre
    similar_tracks = genre_filtered_df[genre_filtered_df['cluster'] == cluster]

    # Exclude the track itself (if needed)
    similar_tracks = similar_tracks[similar_tracks.index != df.index[track_index]]

    # Recommend the top 'num_recommendations' tracks
    recommendations = similar_tracks.head(num_recommendations)

    return recommendations[['track_name', 'track_genre', 'tempo_std', 'danceability_std']]

# Interactive prompt to enter genre
genre_input = input("Please enter the genre you are interested in (e.g., Rock, Pop, Hip-Hop): ")

# Example: Recommend tracks similar to a specific track with index = 5
track_index = 5  # Track index for example
recommendations = recommend_tracks_by_genre_and_cluster(track_index, genre_input, df_top_genres_old, num_recommendations=5)

print(f"Recommendations for Track with index {track_index} in Genre '{genre_input}':\n")
print(recommendations)

# Optional: Visualize the clusters again
plt.figure(figsize=(8, 6))
plt.scatter(
    df_top_genres_old['tempo_std'],
    df_top_genres_old['danceability_std'],
    c=df_top_genres_old['cluster'],
    cmap='viridis'
)
plt.title("K-means Clustering of Tracks by Tempo and Danceability")
plt.xlabel('Tempo')
plt.ylabel('Danceability')
plt.colorbar(label='Cluster')
plt.show()

##K-means Clustering of Tracks by Loudness and Energy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Assuming df_top_genres_old contains the necessary features and clusters
k = 3  # Number of clusters
kmeans = KMeans(n_clusters=k, random_state=42)

# Fit the K-means model
df_top_genres_old['cluster'] = kmeans.fit_predict(df_top_genres_old[['loudness_std', 'energy_std']])

# Function to get recommendations based on track index, genre, and cluster
def recommend_tracks_by_genre_and_cluster(track_index, track_genre, df, num_recommendations=5):
    """
    Recommends tracks based on the genre and the cluster of a given track index.

    Args:
        track_index: The index of the track in the DataFrame.
        genre: The genre to filter the tracks.
        df: The DataFrame containing track data and clusters.
        num_recommendations: The number of recommendations to generate.

    Returns:
        A DataFrame containing the recommended tracks.
    """
    # Filter the DataFrame by the specified genre
    genre_filtered_df = df[df['track_genre'].str.lower() == track_genre.lower()]

    if genre_filtered_df.empty:
        return f"No tracks found for genre: {track_genre}"

    # Get the cluster of the input track using the index
    cluster = df.loc[df.index[track_index], 'cluster']

    # Filter tracks in the same cluster within the selected genre
    similar_tracks = genre_filtered_df[genre_filtered_df['cluster'] == cluster]

    # Exclude the track itself (if needed)
    similar_tracks = similar_tracks[similar_tracks.index != df.index[track_index]]

    # Recommend the top 'num_recommendations' tracks
    recommendations = similar_tracks.head(num_recommendations)

    return recommendations[['track_name', 'track_genre', 'loudness_std', 'energy_std']]

# Interactive prompt to enter genre
genre_input = input("Please enter the genre you are interested in (e.g., Rock, Pop, Hip-Hop): ")

# Example: Recommend tracks similar to a specific track with index = 5
track_index = 5  # Track index for example
recommendations = recommend_tracks_by_genre_and_cluster(track_index, genre_input, df_top_genres_old, num_recommendations=5)

print(f"Recommendations for Track with index {track_index} in Genre '{genre_input}':\n")
print(recommendations)

# Optional: Visualize the clusters again
plt.figure(figsize=(8, 6))
plt.scatter(
    df_top_genres_old['danceability_std'],
    df_top_genres_old['energy_std'],
    c=df_top_genres_old['cluster'],
    cmap='viridis'
)
plt.title("K-means Clustering of Tracks by Loudness and Energy")
plt.xlabel('Energy')
plt.ylabel('Loudness')
plt.colorbar(label='Cluster')
plt.show()

##K-means Clustering of Tracks by Valence and Danceability

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Assuming df_top_genres_old contains the necessary features and clusters
k = 3  # Number of clusters
kmeans = KMeans(n_clusters=k, random_state=42)

# Fit the K-means model
df_top_genres_old['cluster'] = kmeans.fit_predict(df_top_genres_old[[ 'danceability_std', 'valence_std']])

# Function to get recommendations based on track index, genre, and cluster
def recommend_tracks_by_genre_and_cluster(track_index, track_genre, df, num_recommendations=5):
    """
    Recommends tracks based on the genre and the cluster of a given track index.

    Args:
        track_index: The index of the track in the DataFrame.
        genre: The genre to filter the tracks.
        df: The DataFrame containing track data and clusters.
        num_recommendations: The number of recommendations to generate.

    Returns:
        A DataFrame containing the recommended tracks.
    """
    # Filter the DataFrame by the specified genre
    genre_filtered_df = df[df['track_genre'].str.lower() == track_genre.lower()]

    if genre_filtered_df.empty:
        return f"No tracks found for genre: {track_genre}"

    # Get the cluster of the input track using the index
    cluster = df.loc[df.index[track_index], 'cluster']

    # Filter tracks in the same cluster within the selected genre
    similar_tracks = genre_filtered_df[genre_filtered_df['cluster'] == cluster]

    # Exclude the track itself (if needed)
    similar_tracks = similar_tracks[similar_tracks.index != df.index[track_index]]

    # Recommend the top 'num_recommendations' tracks
    recommendations = similar_tracks.head(num_recommendations)

    return recommendations[['track_name', 'track_genre', 'valence_std', 'danceability_std']]

# Interactive prompt to enter genre
genre_input = input("Please enter the genre you are interested in: ")

# Example: Recommend tracks similar to a specific track with index = 5
track_index = 5  # Track index for example
recommendations = recommend_tracks_by_genre_and_cluster(track_index, genre_input, df_top_genres_old, num_recommendations=5)

print(f"Recommendations for Track with index {track_index} in Genre '{genre_input}':\n")
print(recommendations)

# Optional: Visualize the clusters again
plt.figure(figsize=(8, 6))
plt.scatter(
    df_top_genres_old['danceability_std'],
    df_top_genres_old['valence_std'],
    c=df_top_genres_old['cluster'],
    cmap='viridis'
)
plt.title("K-means Clustering of Tracks by Valence and Danceability")
plt.xlabel('Danceability')
plt.ylabel('Valence')
plt.colorbar(label='Cluster')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Assuming df_top_genres_old contains the necessary features and clusters
k = 4  # Number of clusters
kmeans = KMeans(n_clusters=k, random_state=42)

# Fit the K-means model
df_top_genres_old['cluster'] = kmeans.fit_predict(df_top_genres_old[['tempo_std', 'danceability_std']])

# Function to get recommendations based on a given track ID
def recommend_tracks(track_index, df, num_recommendations=5):
    """
    Recommends tracks based on the cluster of a given track index.

    Args:
        track_index: The index of the track in the DataFrame.
        df: The DataFrame containing track data and clusters.
        num_recommendations: The number of recommendations to generate.

    Returns:
        A DataFrame containing the recommended tracks.
    """
    # Get the cluster of the input track using the index
    cluster = df.loc[df.index[track_index], 'cluster']

    # Filter tracks in the same cluster
    similar_tracks = df[df['cluster'] == cluster]

    # Exclude the track itself (if needed)
    similar_tracks = similar_tracks[similar_tracks.index != df.index[track_index]]

    # Recommend the top 'num_recommendations' tracks
    recommendations = similar_tracks.head(num_recommendations)

    return recommendations[['track_name', 'tempo_std', 'danceability_std']]

# Example: Recommend tracks similar to a specific track with index = 5
track_index = 5  # Using index instead of track_name
recommendations = recommend_tracks(track_index, df_top_genres_old, num_recommendations=5)

print(f"Recommendations for Track with index {track_index}:\n")
print(recommendations)

# Optional: Visualize the clusters again
plt.figure(figsize=(8, 6))
plt.scatter(
    df_top_genres_old['tempo_std'],
    df_top_genres_old['danceability_std'],
    c=df_top_genres_old['cluster'],
    cmap='viridis'
)
plt.title("K-means Clustering of Tracks by Tempo and Danceability")
plt.xlabel('Tempo (Standardized)')
plt.ylabel('Danceability (Standardized)')
plt.colorbar(label='Cluster')
plt.show()

"""Refined Cluster Descriptions
Cluster 0 (Yellow):

High Tempo, High Danceability
Likely includes dance tracks or energetic party music, suitable for clubs or energetic environments.
Cluster 1 (Purple):

Low Tempo, Low Danceability
Likely includes slow ballads, acoustic tracks, or calming music. These are softer and less rhythmic, appealing to listeners who prefer relaxing or emotional music.
Cluster 2 (Teal/Green):

Medium Tempo, Medium Danceability
Likely includes tracks with a balance of rhythm and energy, such as pop or mid-tempo rock tracks. These may appeal to a broader audience.
Cluster 3 (Blue):

High Tempo, Low Danceability
Includes fast-paced but less danceable music, such as instrumental, experimental, or upbeat tracks with complex rhythms.
"""



"""Jeny Sherchan's latest work

Analyze the dominant genres and musical features
"""

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import pandas as pd

# Using df_v11_genre_list for analysis
df_analysis = df_v12_genre_list_standard

# 1. Analyze Dominant Genres
# Combine all genre lists into one
all_genres = df_analysis['track_genre'].explode()

# Count the frequency of each genre
genre_counts = Counter(all_genres)

# Create a DataFrame for visualization
genre_df = pd.DataFrame(genre_counts.items(), columns=['Genre', 'Count']).sort_values(by='Count', ascending=False)

# Plot the top 10 genres
plt.figure(figsize=(12, 6))
sns.barplot(data=genre_df.head(10), x='Genre', y='Count')
plt.title('Top 10 Dominant Genres')
plt.ylabel('Count')
plt.xlabel('Genre')
plt.xticks(rotation=45)
plt.show()

# 2. Analyze Musical Features
musical_features = ['danceability_std', 'energy_std', 'tempo_std', 'acousticness_std', 'valence_std', 'popularity_std']

# Plot distributions of musical features
for feature in musical_features:
    plt.figure(figsize=(8, 4))
    sns.histplot(df_analysis[feature], kde=True, bins=30)
    plt.title(f'Distribution of {feature.capitalize()}')
    plt.xlabel(feature.capitalize())
    plt.ylabel('Frequency')
    plt.show()

"""Compare the artist's tracks to similar genres"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Select artist's tracks
artist_name = "Bruno Mars"  # Replace with the target artist's name
artist_tracks = df_v10_clean_standard[df_v10_clean_standard['artists'].str.contains(artist_name, case=False, na=False)]

# Aggregate data by genre
genre_features = ['danceability_std', 'energy_std', 'tempo_std', 'acousticness_std', 'valence_std', 'popularity_std']
genre_aggregates = df_v10_clean_standard.groupby('track_genre')[genre_features].mean().reset_index()

# Identify genres associated with the artist
artist_genres = artist_tracks['track_genre'].unique()

# Filter the genre aggregates to include only similar genres
similar_genres = genre_aggregates[genre_aggregates['track_genre'].isin(artist_genres)]

# Calculate average features for the artist's tracks
artist_feature_avg = artist_tracks[genre_features].mean()

# Visualize comparisons
for feature in genre_features:
    plt.figure(figsize=(8, 4))

    # Plot genre averages
    plt.bar(similar_genres['track_genre'], similar_genres[feature], label='Genre Average', alpha=0.7)

    # Plot artist's average as a line
    plt.axhline(artist_feature_avg[feature], color='red', linestyle='--', label=f"{artist_name}'s Average")

    plt.title(f"Comparison of {feature.capitalize()} - {artist_name} vs. Similar Genres")
    plt.ylabel(feature.capitalize())
    plt.xlabel("Genre")
    plt.xticks(rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Define a threshold for high-performing songs
popularity_threshold = 80  # Adjust as needed
high_performing_songs = df_v9_clean_df[df_v9_clean_df['popularity'] >= popularity_threshold]

# Features to analyze
features_to_analyze = ['loudness', 'energy', 'danceability', 'valence', 'tempo']

# 1. Popularity vs Feature Analysis
for feature in features_to_analyze:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=df_v9_clean_df, x=feature, y='popularity', alpha=0.6, label='All Songs')
    sns.scatterplot(data=high_performing_songs, x=feature, y='popularity', color='red', label='High-Performing Songs')
    plt.title(f'Popularity vs {feature.capitalize()}')
    plt.xlabel(feature.capitalize())
    plt.ylabel('Popularity')
    plt.legend()
    plt.tight_layout()
    plt.show()

# 2. Trends in Features for High-Performing Songs
# Sort songs by popularity
sorted_songs = high_performing_songs.sort_values(by='popularity', ascending=False)

# Plot trends for each feature
for feature in features_to_analyze:
    plt.figure(figsize=(10, 6))
    plt.plot(sorted_songs['popularity'], sorted_songs[feature], marker='o', label=feature.capitalize())
    plt.title(f'Trend of {feature.capitalize()} vs Popularity')
    plt.xlabel('Popularity')
    plt.ylabel(feature.capitalize())
    plt.legend()
    plt.tight_layout()
    plt.show()

"""Jeny Sherchan's work

"""

# Example scatter plot for clusters using two features

# Import KMeans
from sklearn.cluster import KMeans

# Prepare features for clustering before plotting
# features should be derived from the filtered DataFrame, not the original
features = df_individualart[['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo']]



kmeans = KMeans(n_clusters=5, random_state=42)  # Assuming 5 clusters
# Fit and predict on the filtered data
df_individualart['cluster'] = kmeans.fit_predict(df_individualart[['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo']])

plt.figure(figsize=(12, 8))
sns.scatterplot(
    x='loudness',
    y='energy',
    hue='cluster',
    data=df_individualart, # Use the filtered data for plotting
    palette='viridis',
    alpha=0.7
)
plt.title('Clustering of Songs Based on Loudness and Energy')
plt.xlabel('Loudness')
plt.ylabel('Energy')
plt.legend(title='Cluster')
plt.show()

# Example scatter plot for clusters using two features

# Import KMeans
from sklearn.cluster import KMeans

# Prepare features for clustering before plotting

# If df_v10_clean_standard is your original DataFrame
df_individualart = df_v10_clean_standard.copy()
# filter for rows where the 'artist' column does not contain ';'
df_individualart = df_individualart[~df_individualart['artists'].str.contains(';')]

# Now you can access the columns:
features = df_individualart[['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo']]

kmeans = KMeans(n_clusters=5, random_state=42)  # Assuming 5 clusters
# Fit and predict on the filtered data
df_individualart['cluster'] = kmeans.fit_predict(df_individualart[['popularity_std', 'danceability_std', 'energy_std', 'loudness_std', 'valence_std', 'tempo_std']])

plt.figure(figsize=(12, 8))
sns.scatterplot(
    x='loudness_std',
    y='energy_std',
    hue='cluster',
    data=df_individualart, # Use the filtered data for plotting
    palette='viridis',
    alpha=0.7
)
plt.title('Clustering of Songs Based on Loudness and Energy')
plt.xlabel('Loudness')
plt.ylabel('Energy')
plt.legend(title='Cluster')
plt.show()

# Example scatter plot for clusters using two features

# Import KMeans
from sklearn.cluster import KMeans

# Prepare features for clustering before plotting
features = df_v10_clean_standard[['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo']]

# Create a boolean mask
mask = ~df_v10_clean_standard['artists'].str.contains(";", regex=False)

# Apply the mask to filter the DataFrame
df_individualart = df_v10_clean_standard[mask]

kmeans = KMeans(n_clusters=5, random_state=42)  # Assuming 5 clusters
df_individualart['cluster'] = kmeans.fit_predict(features) # This line adds the 'cluster' column

plt.figure(figsize=(12, 8))
sns.scatterplot(
    x='loudness',
    y='energy',
    hue='cluster',
    data=df_filtered,
    palette='viridis',
    alpha=0.7
)
plt.title('Clustering of Songs Based on Danceability and Energy')
plt.xlabel('Loudness')
plt.ylabel('Energy')
plt.legend(title='Cluster')
plt.show()

# Example scatter plot for clusters using two features

# Import KMeans
from sklearn.cluster import KMeans

# Prepare features for clustering before plotting
# Use the filtered DataFrame for features
features = df_individualart[['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo']]

# Create a boolean mask
mask = ~df_v10_clean_standard['artists'].str.contains(";", regex=False)

# Apply the mask to filter the DataFrame
df_individualart = df_v10_clean_standard[mask]

kmeans = KMeans(n_clusters=5, random_state=42)  # Assuming 5 clusters
#Fit and predict on the filtered data (df_individualart) using the filtered features
df_individualart['cluster'] = kmeans.fit_predict(df_individualart[['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo']])

plt.figure(figsize=(12, 8))
sns.scatterplot(
    x='loudness',
    y='energy',
    hue='cluster',
    data=df_individualart, # Use the filtered data for plotting
    palette='viridis',
    alpha=0.7
)
plt.title('Clustering of Songs Based on Danceability and Energy')
plt.xlabel('Loudness')
plt.ylabel('Energy')
plt.legend(title='Cluster')
plt.show()

# Example scatter plot for clusters using two features

# Import KMeans
from sklearn.cluster import KMeans

# Prepare features for clustering before plotting
# Use the filtered DataFrame for features
features = df_individualart[['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo']]

# Create a boolean mask
mask = ~df_v10_clean_standard['artists'].str.contains(";", regex=False)

# Apply the mask to filter the DataFrame
df_individualart = df_v10_clean_standard[mask]

kmeans = KMeans(n_clusters=5, random_state=42)  # Assuming 5 clusters
# Fit and predict on the filtered data (df_individualart)
df_individualart['cluster'] = kmeans.fit_predict(df_individualart[['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo']]) # Use the same DataFrame for fit_predict

plt.figure(figsize=(12, 8))
sns.scatterplot(
    x='loudness',
    y='energy',
    hue='cluster',
    data=df_individualart, # Use the filtered data for plotting
    palette='viridis',
    alpha=0.7
)
plt.title('Clustering of Songs Based on Loudness and Energy')
plt.xlabel('Loudness')
plt.ylabel('Energy')
plt.legend(title='Cluster')
plt.show()

##Calculate Cluster Centroids:
import numpy as np

# Calculate cluster centroids
centroids = kmeans.cluster_centers_
centroid_df = pd.DataFrame(centroids, columns=['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo'])

# Print centroids
print(centroid_df)

# Plot centroids
centroid_df.plot(kind='bar', figsize=(12, 6), colormap='viridis')
plt.title('Cluster Centroids - Average Feature Values')
plt.xlabel('Cluster')
plt.ylabel('Feature Value')
plt.legend(loc='upper right', title='Features')
plt.xticks(ticks=np.arange(len(centroids)), labels=[f'Cluster {i}' for i in range(len(centroids))])
plt.tight_layout()
plt.show()

#Data Cleaning:
#df_individualart = df_filtered.copy() # The original line causing the error

# Instead of using df_filtered, which is not defined,
# we will use df_v10_clean_standard as the source DataFrame
df_individualart = df_v10_clean_standard.copy()
df_individualart = df_individualart[~df_individualart['artists'].str.contains(';')] # filter for rows where the 'artist' column does not contain ';'
df_individualart

mask

"""### Analysis and Insights from the Clustering Visualization:

1. **Clustering Based on Danceability and Energy**:
   - The clusters are distributed across the danceability and energy spectrum, ranging from low to high values.
   - Clusters show overlapping but distinct patterns:

2. **Patterns in Clusters**:
   - Songs with high danceability and energy tend to form smaller, tighter clusters, suggesting they cater to niche tastes like party or workout playlists.
   - Lower-energy and less-danceable songs have more widespread clusters, indicating diversity in slower, mellow genres.

3. **Potential Genre Insights**:
   - If genres were labeled on this chart, high-energy, high-danceability clusters (e.g., Cluster 3) would likely correspond to genres like EDM, dance-pop, or hip-hop.
   - Low-energy and low-danceability clusters might represent acoustic, classical, or ballads.

4. **Cluster Overlap**:
   - Some clusters overlap, particularly in moderate ranges of energy and danceability. This overlap could indicate genre blending, such as pop-rock or electronic-acoustic hybrids.

---

### Recommendations:

1. **Personalized Recommendations**:
   - Leverage these clusters for personalized playlists. For instance:
     - **Cluster 3** can target users seeking upbeat tracks for exercise or parties.
     - **Cluster 0** can focus on calm, relaxing playlists for study or unwinding.

2. **Refine Recommendation System**:
   - Consider adding genre and popularity as filters to improve the recommendation model. Users may prefer specific genres within each cluster.

3. **User Segmentation**:
   - Group users based on their preference for specific clusters (e.g., users who listen to Cluster 3 for upbeat music).
   - Offer playlists catering to user moods or activities.

4. **Product Insights**:
   - High-energy and high-danceability songs (e.g., Cluster 3) could inform marketing strategies targeting younger audiences or event organizers.
   - Low-energy clusters (e.g., Cluster 0) could be marketed to audiences seeking relaxation or focus-oriented playlists.

5. **Future Exploration**:
   - Add more variables such as tempo, valence (positivity), or explicit content to refine clusters.
   - Perform sentiment analysis on track names or lyrics to see if certain words correlate with clusters.

---

### Assumptions:

1. **Cluster Relevance**:
   - The clustering assumes that danceability and energy are primary drivers of song similarity. Adding more features like valence or explicit content could provide better-defined clusters.

2. **Diversity within Clusters**:
   - Overlap suggests diversity within clusters, meaning not all songs in a cluster are entirely similar. This could be due to the limitation of features used for clustering.

3. **User Preferences**:
   - The analysis assumes that clusters reflect meaningful preferences, such as mood-based listening. This assumption may vary by demographic or context.
The **personalized recommendations** from this clustering analysis can cater to various **types of users or personas**, depending on their preferences, activities, and listening habits. Here's a breakdown of the possible target audiences:

---
The personalized recommendations from this clustering analysis can cater to various types of users or personas, depending on their preferences, activities, and listening habits. Here's a breakdown of the possible target audiences:

**1. Mood-Based Listeners**
   - **Cluster 3 (High Energy, High Danceability)**:
     - Users looking for **party playlists**, **exercise tracks**, or songs for upbeat environments.
     - Ideal for younger audiences or people preparing for events like parties or workouts.
   - **Cluster 0 (Low Energy, Low Danceability)**:
     - Users seeking **calm, relaxing playlists**, such as for studying, reading, or sleeping.
     - Catered to users who prefer acoustic, chill, or classical genres.

---

**2. Genre Enthusiasts**
   - Each cluster likely correlates with specific genres:
     - **Cluster 3** could focus on **EDM, hip-hop, or pop** genres.
     - **Cluster 0** might target users favoring **acoustic, ballads, or classical genres**.
   - Personalized recommendations could provide tracks within their favorite genre based on their energy and danceability preferences.

---

**3. Activity-Based Listeners**
   - **For Workouts or Running**:
     - Songs from high-energy clusters (e.g., Cluster 3 or Cluster 1) provide rhythmic, motivating music.
   - **For Relaxation**:
     - Tracks from low-energy clusters (e.g., Cluster 0) help users unwind or relax after a long day.
   - **For Social Gatherings**:
     - Danceable songs from high-energy clusters (e.g., Cluster 3) can be used to create party playlists.

---

**4. Passive Listeners**
   - These users don’t actively choose music but rely on recommendations.
   - The system can suggest tracks similar to songs they’ve liked before, leveraging the cluster their favorite songs belong to.

---

**5. Explorative Listeners**
   - Users who enjoy discovering new music across different genres or styles.
   - For instance, recommendations could expose them to songs from the same cluster but in different genres.

---

**6. Platform-Specific Strategies**
   - **For Streaming Platforms**:
     - Clusters can help tailor recommendations to user profiles, such as "Discover Weekly" or "Made For You" playlists.
   - **For Marketing or Ads**:
     - High-energy tracks might appeal to fitness brands or party-themed campaigns.
     - Low-energy songs might align with meditation apps or self-care products.

---

**7. Music Creators or Producers**
   - Recommendations can help artists identify the kind of tracks their audience enjoys.
   - Clustering could suggest trends in energy and danceability, inspiring new music creation.

---

**Conclusion**
The recommendations can be tailored for **listeners**, **streaming platforms**, or **marketers**, depending on the use case.

Cluster Comparison:

Cluster 0:
Lowest popularity and energy.
Moderate tempo and valence.
Likely slow-paced, mellow tracks.

Cluster 1:
Higher popularity with balanced energy, danceability, and valence.
Moderate tempo, catering to casual listeners.

Cluster 2:
High popularity and danceability.
Slightly higher energy and valence, suggesting uplifting tracks.


assumption: so many genres in a certain cluster- for eg. sad, piano??

Cluster 3:
High valence (happy, positive songs) with moderate danceability and tempo.
Likely feel-good tracks suited for relaxing moments.

Cluster 4:
High tempo, low popularity, and valence.
Likely niche or high-intensity tracks for a specific audience.
"""

# @title Default title text
from seaborn import pairplot

# Sample 1000 points for a clearer visualization
sampled_data = df_v10_clean_standard.sample(1000, random_state=42)

pairplot_data = sampled_data[['popularity', 'danceability', 'energy', 'valence', 'tempo', 'cluster']]
sns.pairplot(pairplot_data, hue='cluster', palette='viridis', diag_kind='kde')
plt.suptitle("Pair Plot of Clusters", y=1.02)
plt.show()

# @title Default title text
from seaborn import pairplot

# Sample 1000 points for a clearer visualization
sampled_data = df_individualart.sample(500, random_state=42)
# dont include popularity?
# create a df top 10 and go to max top 100 of songs
pairplot_data = sampled_data[['popularity', 'danceability', 'energy', 'valence', 'tempo', 'cluster']]
sns.pairplot(pairplot_data, hue='cluster', palette='viridis', diag_kind='kde')
plt.suptitle("Pair Plot of Clusters", y=1.02)
plt.show()

import numpy as np

# Calculate cluster centroids
centroids = kmeans.cluster_centers_
centroid_df = pd.DataFrame(centroids, columns=['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo'])

# Plot centroids
centroid_df.plot(kind='bar', figsize=(12, 6), colormap='viridis')
plt.title('Cluster Centroids - Average Feature Values')
plt.xlabel('Cluster')
plt.ylabel('Feature Value')
plt.legend(loc='upper right', title='Features')
plt.xticks(ticks=np.arange(len(centroids)), labels=[f'Cluster {i}' for i in range(len(centroids))])
plt.tight_layout()
plt.show()

cluster_counts = df_individualart['cluster'].value_counts()

# Plot
plt.figure(figsize=(8, 6))
sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='viridis')
plt.title('Number of Songs in Each Cluster')
plt.xlabel('Cluster')
plt.ylabel('Number of Songs')
plt.xticks(ticks=range(len(cluster_counts)), labels=[f'Cluster {i}' for i in range(len(cluster_counts))])
plt.show()

cluster_counts = df_v10_clean_standard['cluster'].value_counts()

# Plot
plt.figure(figsize=(8, 6))
sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='viridis')
plt.title('Number of Songs in Each Cluster')
plt.xlabel('Cluster')
plt.ylabel('Number of Songs')
plt.xticks(ticks=range(len(cluster_counts)), labels=[f'Cluster {i}' for i in range(len(cluster_counts))])
plt.show()

"""### **Analysis and Insights**

1. **Cluster Distribution**:
   - **Cluster 1** has the highest number of songs, indicating that songs with moderate popularity, energy, and tempo dominate the dataset.
   - **Cluster 3** also has a significant number of songs, likely representing tracks with higher valence (positive mood) and moderate danceability.
   - **Cluster 4** contains the fewest songs, suggesting that high-tempo, niche tracks are less common.

2. **Popularity Spread**:
   - The majority of songs fall into Clusters 1 and 3, indicating a general preference for tracks with balanced features.
   - Clusters 0 and 4 have fewer songs, suggesting these are either niche tracks or less commercially appealing.

3. **Assumptions**:
   - Songs in **Cluster 1** are likely mainstream, appealing to a broader audience.
   - Songs in **Cluster 4** may target specific listeners who prefer fast-paced, high-tempo tracks.
   - **Cluster 0** might represent slow-paced or low-energy tracks catering to a more relaxed audience.

---

### **Recommendations**

1. **For Streaming Platforms**:
   - Focus playlists around **Cluster 1** and **Cluster 3** to appeal to a wide range of listeners.
   - Highlight niche playlists (e.g., workout or high-energy playlists) using songs from **Cluster 4**.

2. **For Artists**:
   - Create tracks aligning with **Cluster 1** characteristics for broader appeal.
   - Explore unique soundscapes in **Cluster 4** to stand out in niche markets.

3. **For Listeners**:
   - **Cluster 1 & 3**: Explore if you enjoy balanced, upbeat tracks with mainstream appeal.
   - **Cluster 4**: Ideal for niche preferences or high-energy activities.
   - **Cluster 0**: Relaxing or low-energy playlists for unwinding.

---

### **Conclusion**

This analysis highlights how song clusters represent diverse audience preferences. Clusters 1 and 3 dominate, suggesting that balanced, positive, and moderately energetic tracks resonate most with listeners. Clusters like 0 and 4 cater to more niche tastes, offering opportunities for targeted personalization and recommendation strategies.
"""

from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Plot 3D scatter
scatter = ax.scatter(
    df_individualart['danceability'],
    df_individualart['energy'],
    df_individualart['valence'],
    c=df_individualart['cluster'],
    cmap='viridis',
    alpha=0.6
)

ax.set_title("3D Visualization of Clusters")
ax.set_xlabel('Danceability')
ax.set_ylabel('Energy')
ax.set_zlabel('Valence')
plt.colorbar(scatter, label='Cluster')
plt.show()

from sklearn.cluster import KMeans

# Prepare features for clustering
features = df_v10_clean_standard[['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo']]
kmeans = KMeans(n_clusters=5, random_state=42)
df_v10_clean_standard['cluster'] = kmeans.fit_predict(features)

# Recommend songs from the same cluster
def recommend_songs_interactive(df):
    first_prompt = True  # Flag to indicate the first prompt

    while True:
        # Prompt the user for the song name
        if first_prompt:
            song_name = input("Enter a song name to get recommendations: ").strip()
            first_prompt = False  # Set the flag to False after the first prompt
        else:
            continue_prompt = input("Do you want to search for another song? (Y/N): ").strip().lower()
            if continue_prompt == 'n':
                print("Exiting the recommendation system. Thank you!")
                break
            elif continue_prompt == 'y':
                song_name = input("Enter another song name: ").strip()
            else:
                print("Invalid input. Please type 'Y' or 'N'.")
                continue

        # Check if the song exists in the DataFrame
        if song_name in df['track_name'].values:
            # Get the cluster, artist, and genre of the original song
            original_song_data = df[df['track_name'] == song_name].iloc[0]
            cluster = original_song_data['cluster']
            original_artist = original_song_data['artists']
            original_genre = original_song_data['track_genre']

            print(f"\nOriginal Song Information:")
            print(f"Track Name: {song_name}")
            print(f"Artist: {original_artist}")
            print(f"Genre: {original_genre}")
            print(f"Cluster: {cluster}")

            # Get recommendations from the same cluster
            recommendations = df[df['cluster'] == cluster][['track_name', 'artists', 'track_genre']]
            recommendations = recommendations[recommendations['track_name'] != song_name]  # Exclude the original song

            if not recommendations.empty:
                print("\nRecommended Songs:")
                print(recommendations.sample(min(5, len(recommendations))))  # Return up to 5 random recommendations
            else:
                print("No recommendations available in the same cluster.")
        else:
            print(f"Song '{song_name}' not found in the dataset.")

# Example usage
recommend_songs_interactive(df_v10_clean_standard)

from sklearn.cluster import KMeans
#df_individualart
# Prepare features for clustering
features = df_individualart[['popularity', 'danceability', 'energy', 'loudness', 'valence', 'tempo']]
kmeans = KMeans(n_clusters=5, random_state=42)
df_individualart['cluster'] = kmeans.fit_predict(features)

# Recommend songs from the same cluster
def recommend_songs_interactive(df):
    first_prompt = True  # Flag to indicate the first prompt

    while True:
        # Prompt the user for the song name
        if first_prompt:
            song_name = input("Enter a song name to get recommendations: ").strip()
            first_prompt = False  # Set the flag to False after the first prompt
        else:
            continue_prompt = input("Do you want to search for another song? (Y/N): ").strip().lower()
            if continue_prompt == 'n':
                print("Exiting the recommendation system. Thank you!")
                break
            elif continue_prompt == 'y':
                song_name = input("Enter another song name: ").strip()
            else:
                print("Invalid input. Please type 'Y' or 'N'.")
                continue

        # Check if the song exists in the DataFrame
        if song_name in df['track_name'].values:
            # Get the cluster, artist, and genre of the original song
            original_song_data = df[df['track_name'] == song_name].iloc[0]
            cluster = original_song_data['cluster']
            original_artist = original_song_data['artists']
            original_genre = original_song_data['track_genre']

            print(f"\nOriginal Song Information:")
            print(f"Track Name: {song_name}")
            print(f"Artist: {original_artist}")
            print(f"Genre: {original_genre}")
            print(f"Cluster: {cluster}")

            # Get recommendations from the same cluster
            recommendations = df[df['cluster'] == cluster][['track_name', 'artists', 'track_genre']]
            recommendations = recommendations[recommendations['track_name'] != song_name]  # Exclude the original song

            if not recommendations.empty:
                print("\nRecommended Songs:")
                print(recommendations.sample(min(5, len(recommendations))))  # Return up to 5 random recommendations
            else:
                print("No recommendations available in the same cluster.")
        else:
            print(f"Song '{song_name}' not found in the dataset.")

# Example usage
recommend_songs_interactive(df_v10_clean_standard)

# Calculate average popularity for each artist
top_artists = df_individualart.groupby('artists')['popularity'].sum().sort_values(ascending=False).head(10)
print("Top 10 Popular Artists:")
print(top_artists)

b

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'top_artists' is already calculated as in your previous code
top_artists = df_individualart.groupby('artists')['popularity'].sum().sort_values(ascending=False).head(10)


plt.figure(figsize=(12, 6))  # Adjust figure size if needed
sns.barplot(x=top_artists.index, y=top_artists.values, palette="viridis")
plt.title("Top 10 Popular Artists")
plt.xlabel("Artist")
plt.ylabel("Total Popularity")
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

# Filter data for the top 5 artists
top_artists_data = df_individualart[df_individualart['artists'].isin(top_artists.index)]

# Find the most popular genres for the top artists
popular_genres = top_artists_data.groupby(['artists', 'track_genre'])['popularity'].sum().reset_index()
popular_genres = popular_genres.sort_values(by=['artists', 'popularity'], ascending=[True, False])

print("Top Genres for Each of the Top 5 Artists:")
print(popular_genres)

import matplotlib.pyplot as plt
import seaborn as sns

# Pivot data for visualization
pivot_data = popular_genres.pivot(index='artists', columns='track_genre', values='popularity').fillna(0)

# Plot the stacked bar chart
pivot_data.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='tab10')
plt.title('Genre Popularity Contribution for Top 5 Artists')
plt.ylabel('Popularity')
plt.xlabel('Artists')
plt.legend(title='Genres', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Filter songs from the top genres for the top artists
top_genres = popular_genres['track_genre'].unique()
songs_in_top_genres = df_v10_clean_standard[df_v10_clean_standard['track_genre'].isin(top_genres)]

# Recommend top 5 songs by popularity
top_songs = songs_in_top_genres.sort_values(by='popularity', ascending=False).head(5)
print("Top Songs in Popular Genres for Top Artists:")
print(top_songs[['track_name', 'artists', 'popularity', 'track_genre']])

def recommend_based_on_artist(artist_name, df):
    # Filter data for the given artist
    artist_data = df[df['artists'].str.contains(artist_name, case=False, na=False)]

    if not artist_data.empty:
        # Identify the artist's top genres
        artist_genres = artist_data['track_genre'].unique()

        # Find songs in similar genres
        genre_recommendations = df[df['track_genre'].isin(artist_genres) & ~df['artists'].str.contains(artist_name, case=False)]

        # Recommend top 5 songs by popularity
        recommendations = genre_recommendations.sort_values(by='popularity', ascending=False).head(5)

        print(f"\nRecommendations for artist '{artist_name}':")
        print(recommendations[['track_name', 'artists', 'popularity', 'track_genre']])
    else:
        print(f"No data found for artist '{artist_name}'.")

# Example usage
artist_name = input("Enter an artist's name for recommendations: ").strip()
recommend_based_on_artist(artist_name, df_v10_clean_standard)